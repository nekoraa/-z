import torch
import torch.nn as nn


class 卷积嵌入层(nn.Module):
    def __init__(self, 嵌入维度, 占位, 隐藏层维度=129):
        super(卷积嵌入层, self).__init__()

        # 定义全连接层，将 (通道数 * 频域高度) 映射到隐藏层维度
        self.全连接层1 = nn.Linear(4 * 129, 隐藏层维度)
        self.relu1 = nn.ReLU()  # 激活函数

        # 第二个全连接层，将隐藏层维度映射到嵌入维度
        self.全连接层2 = nn.Linear(隐藏层维度, 嵌入维度)
        self.relu2 = nn.ReLU()  # 激活函数

        # 第三个全连接层 (可选), 将嵌入维度进一步处理
        self.全连接层3 = nn.Linear(嵌入维度, 嵌入维度)
        self.relu3 = nn.ReLU()  # 激活函数

    def forward(self, 输入):
        批次大小, 通道数, 频域高度, 时间长 = 输入.shape

        # 保持4D张量形状 (批次大小, 1, 通道数 * 频域高度, 时间长)
        输入 = 输入.view(批次大小, 1, 通道数 * 频域高度, 时间长)

        # 重排维度，将其从 (批次大小, 1, 通道数 * 频域高度, 时间长)
        # 转换为 (批次大小, 时间长, 通道数 * 频域高度)
        输出张量 = 输入.permute(0, 3, 1, 2).reshape(批次大小, 时间长, 通道数 * 频域高度).squeeze(1)

        # 第一层全连接 + 激活函数
        输出张量 = self.relu1(self.全连接层1(输出张量))

        # 第二层全连接 + 激活函数
        输出张量 = self.relu2(self.全连接层2(输出张量))

        # 第三层全连接 + 激活函数 (如果需要进一步处理)
        输出张量 = self.relu3(self.全连接层3(输出张量))

        return 输出张量


# 假设输入是 [批次大小, 4, 129, 时间长度]，例如 (16, 4, 129, 100)
批次大小 = 16
时间长度 = 103
输入 = torch.randn(批次大小, 4, 129, 时间长度)

# 设置嵌入维度
嵌入维度 = 128

# 初始化模型
模型 = 卷积嵌入层(嵌入维度,1)

# 前向传播
输出 = 模型(输入)

print(输出.shape)  # 输出形状应为 [批次大小, 嵌入维度, 时间长度]
